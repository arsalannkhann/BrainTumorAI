# Neuro-HyMamba Advanced Extensions - Implementation Plan

Implementing the four recommended extensions from the technical report to push classification accuracy toward the 99% target.

## Overview

The current implementation provides a solid foundation with ConvNeXt/Swin backbones and attention-based slice aggregation. These extensions add:

1. **Vision Mamba (Vim)**: State-space model backbone with linear complexity for long-range dependencies
2. **SegMamba**: Tri-Oriented Mamba for 3D segmentation with global context
3. **TransMIL**: Transformer-based MIL aggregation replacing LSTM-style attention pooling
4. **BM-MAE**: Self-supervised pretraining for modality-invariant representations

> [!IMPORTANT]
> Vision Mamba requires `mamba-ssm` package which needs CUDA compilation. The implementation includes fallback options when Mamba is unavailable.

---

## Proposed Changes

### New Mamba Module

#### [NEW] [mamba/__init__.py](file:///home/ubuntu/BrainTumor/mamba/__init__.py)
Package initialization with conditional imports for mamba-ssm.

#### [NEW] [vim_backbone.py](file:///home/ubuntu/BrainTumor/mamba/vim_backbone.py)
Vision Mamba backbone with bidirectional state-space modeling:
- `Mamba2DBlock`: Core SSM block with selective scanning
- `VisionMamba`: Full backbone with patch embedding and Mamba layers
- `VimClassifier`: Wrapper compatible with existing [BrainTumorClassifier](file:///home/ubuntu/BrainTumor/classification/model.py#87-188)

#### [NEW] [segmamba.py](file:///home/ubuntu/BrainTumor/mamba/segmamba.py)
SegMamba U-shape architecture for 3D segmentation:
- `TriOrientedMamba`: Processes axial/coronal/sagittal sequences
- `MambaEncoder`: Hierarchical encoder with Mamba blocks
- `SegMamba`: Full U-Net style architecture with skip connections

---

### TransMIL Aggregation

#### [NEW] [transmil.py](file:///home/ubuntu/BrainTumor/classification/transmil.py)
Transformer-based Multiple Instance Learning:
- `PPEG`: Pyramid Position Encoding Generator for spatial order
- `TransMILAggregator`: Multi-head self-attention over slice features
- Drop-in replacement for [MultiSliceEncoder](file:///home/ubuntu/BrainTumor/classification/model.py#14-85) attention aggregation

---

### BM-MAE Pretraining

#### [NEW] [pretraining/__init__.py](file:///home/ubuntu/BrainTumor/pretraining/__init__.py)
Package initialization.

#### [NEW] [bm_mae.py](file:///home/ubuntu/BrainTumor/pretraining/bm_mae.py)
Brain Multimodal Masked Autoencoder:
- `BM_MAE`: Encoder-decoder with high mask ratio (75%)
- Modality dropout for handling missing modalities
- Patch-wise reconstruction loss

#### [NEW] [train_mae.py](file:///home/ubuntu/BrainTumor/pretraining/train_mae.py)
Pretraining script with:
- Unlabeled data loading
- MAE training loop with AMP
- Checkpoint saving for downstream fine-tuning

---

### Model Integration

#### [MODIFY] [model.py](file:///home/ubuntu/BrainTumor/classification/model.py)
Add Vim and TransMIL support:
- New `VimBrainTumorClassifier` class using Vision Mamba backbone
- `TransMILEncoder` option for multi-slice aggregation
- Update [create_classifier](file:///home/ubuntu/BrainTumor/classification/model.py#240-276) factory function

#### [MODIFY] [model.py](file:///home/ubuntu/BrainTumor/segmentation/model.py)
Add SegMamba model option:
- Import and integrate `SegMamba` architecture
- Update [SegmentationModel](file:///home/ubuntu/BrainTumor/segmentation/model.py#150-218) wrapper to support "segmamba" option

---

### Configuration Updates

#### [MODIFY] [cls.yaml](file:///home/ubuntu/BrainTumor/configs/cls.yaml)
Add new model options:
```yaml
model:
  backbone: "vim_base"  # or "convnext_base", "swin_base"
  aggregation: "transmil"  # or "attention", "mean", "max"
  pretrained_mae: null  # path to MAE checkpoint
```

#### [MODIFY] [seg.yaml](file:///home/ubuntu/BrainTumor/configs/seg.yaml)
Add SegMamba option:
```yaml
model:
  name: "SegMamba"  # or "UNet", "UNETR", "SwinUNETR"
```

#### [NEW] [mae.yaml](file:///home/ubuntu/BrainTumor/configs/mae.yaml)
MAE pretraining configuration:
- Mask ratio, patch size, encoder/decoder dimensions
- Pretraining epochs, learning rate schedule

---

### Pipeline Updates

#### [MODIFY] [run_pipeline.sh](file:///home/ubuntu/BrainTumor/run_pipeline.sh)
Add pretraining stage:
- `mae-pretrain`: Self-supervised pretraining on unlabeled data
- Pass MAE checkpoint to classification training

#### [MODIFY] [requirements.txt](file:///home/ubuntu/BrainTumor/requirements.txt)
Add dependencies:
```
mamba-ssm>=2.0.0  # State-space models
causal-conv1d>=1.0.0  # Required by mamba-ssm
einops>=0.7.0  # Tensor operations
```

---

## Verification Plan

### Automated Tests

#### Unit Tests: Module Instantiation
Commands to verify each new module:
```bash
# Test Vision Mamba backbone
python -c "from mamba.vim_backbone import VisionMamba, VimClassifier; print('Vim OK')"

# Test SegMamba
python -c "from mamba.segmamba import SegMamba; print('SegMamba OK')"

# Test TransMIL
python -c "from classification.transmil import TransMILAggregator; print('TransMIL OK')"

# Test BM-MAE
python -c "from pretraining.bm_mae import BM_MAE; print('BM-MAE OK')"
```

#### Integration Tests: Forward Pass
Each new module includes a `__main__` test block:
```bash
# Test Vim forward pass
python -m mamba.vim_backbone

# Test SegMamba forward pass  
python -m mamba.segmamba

# Test TransMIL aggregation
python -m classification.transmil

# Test BM-MAE reconstruction
python -m pretraining.bm_mae
```

#### Existing Model Tests
Run existing test blocks to ensure no regressions:
```bash
python -m classification.model
python -m segmentation.model
python -m classification.loss
```

### Manual Verification

1. **Visual inspection**: After running module tests, verify printed output shows correct tensor shapes
2. **Memory check**: Run `nvidia-smi` during forward pass to confirm GPU memory usage is reasonable
3. **Config validation**: Load updated YAML configs and verify new options parse correctly:
   ```bash
   python -c "import yaml; print(yaml.safe_load(open('configs/cls.yaml'))['model'])"
   ```

---

## Architecture Diagram

```mermaid
flowchart TD
    subgraph Preprocessing
        A[Raw MRI] --> B[N4 Bias Correction]
        B --> C[Z-Score Normalization]
    end
    
    subgraph Segmentation
        C --> D{Model Choice}
        D -->|UNet/UNETR| E1[Standard Encoder]
        D -->|SegMamba| E2[Tri-Oriented Mamba]
        E1 --> F[Tumor Mask]
        E2 --> F
    end
    
    subgraph "ROI + Classification"
        F --> G[ROI Extraction]
        G --> H{Backbone}
        H -->|ConvNeXt/Swin| I1[CNN Features]
        H -->|Vision Mamba| I2[SSM Features]
        I1 --> J{Aggregation}
        I2 --> J
        J -->|Attention| K1[Weighted Pool]
        J -->|TransMIL| K2[Self-Attention MIL]
        K1 --> L[Prediction]
        K2 --> L
    end
    
    subgraph "Optional Pretraining"
        P[Unlabeled MRI] --> Q[BM-MAE]
        Q -.->|Pretrained Weights| H
    end

    style E2 fill:#e8f5e9
    style I2 fill:#e8f5e9
    style K2 fill:#e8f5e9
    style Q fill:#fff3e0
```

---

## Hardware Considerations

| Component | Batch Size | Est. Memory |
|-----------|------------|-------------|
| Vision Mamba (2.5D) | 8 | ~10 GB |
| SegMamba (128Â³) | 2 | ~14 GB |
| TransMIL (16 slices) | 8 | ~8 GB |
| BM-MAE Pretrain | 4 | ~12 GB |

> [!NOTE]
> All memory estimates are for NVIDIA A10G (24 GB). Batch sizes can be reduced with gradient accumulation for smaller GPUs.
